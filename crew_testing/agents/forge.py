"""
ðŸ”§ Forge - Test Infrastructure Engineer

Tagline: "Forging quality, one test at a time"

Role: Test code generator and infrastructure maintainer

Responsibilities:
- Generate pytest test files from scenarios and fixtures
- Generate api_client methods for untested endpoints
- Write syntactically correct Python code
- Follow pytest best practices
- Maintain test infrastructure

Uses CodeGeneratorTool and FileWriterTool to create test files.
"""

from crewai import Agent

from crew_testing.config import CrewConfig
from crew_testing.tools import CodeGeneratorTool, FileWriterTool, OpenAPIAuthDetectorTool


def create_forge_agent() -> Agent:
    """
    Create the Forge agent for test code generation

    Uses GPT-4o for complex code generation reasoning

    Returns:
        Configured CrewAI Agent
    """
    # Get LLM for this agent from config
    llm = CrewConfig.get_llm_for_agent("forge")

    # Initialize tools
    tools = [
        OpenAPIAuthDetectorTool(),  # NEW: Detect auth requirements
        CodeGeneratorTool(),
        FileWriterTool(),
    ]

    # Create agent
    agent = Agent(
        role="Test Infrastructure Engineer",
        goal=(
            "Generate clean, maintainable pytest test code that follows best practices. "
            "Create test files that are easy to read, well-documented, and properly structured."
        ),
        backstory=(
            "You are a master Python developer specializing in pytest and test infrastructure. "
            "You've written thousands of test files and know every pytest best practice. "
            "\n\n"
            "Your code is characterized by:\n"
            "1. **Clarity**: Clear test names, good docstrings, readable assertions\n"
            "2. **DRY Principle**: Reusable fixtures, no code duplication\n"
            "3. **Best Practices**: Proper use of fixtures, markers, parametrize\n"
            "4. **Proper Structure**: Arrange-Act-Assert pattern\n"
            "5. **Good Documentation**: Docstrings explaining what each test verifies\n"
            "\n\n"
            "You understand the MT testing ecosystem:\n"
            "- Tests live in `backend/tests/`\n"
            "- API client lives in `backend/api_client/`\n"
            "- Fixtures should be reusable across multiple tests\n"
            "- Always import from the right modules\n"
            "\n\n"
            "You generate code that:\n"
            "- Uses proper type hints\n"
            "- Has comprehensive docstrings\n"
            "- Follows PEP 8 style guide\n"
            "- Validates syntax before writing\n"
            "- Creates backups before overwriting files\n"
            "\n\n"
            "You're not just generating code - you're building test infrastructure that "
            "developers will rely on for years."
        ),
        verbose=CrewConfig.VERBOSE,
        llm=llm,
        tools=tools,
        max_iter=25,  # Increased for complete test file generation
        allow_delegation=False,
    )

    return agent


def get_forge_task_description() -> str:
    """
    Get the task description for the Forge agent

    Returns:
        Task description string
    """
    return """
Generate pytest test files based on test scenarios and fixtures.

Your task:
1. Review test scenarios from ARCHITECT
2. Review fixtures from MOCKER
3. Generate a complete pytest test file with:
   - Proper imports
   - Fixture definitions
   - Test functions for each scenario
   - Clear docstrings
   - Proper assertions
4. Write the test file to `backend/tests/test_{endpoint_name}.py`

Input from ARCHITECT:
{architect_output}

Input from MOCKER:
{mocker_output}

Output the generated test file path and summary:
{{
    "test_file": "backend/tests/test_matches.py",
    "summary": {{
        "total_tests": 15,
        "total_fixtures": 8,
        "lines_of_code": 245,
        "coverage_areas": ["happy_path", "error", "edge_case", "security", "performance"]
    }},
    "file_written": true
}}

Generate REAL, working Python code. Follow pytest best practices:
- Use `@pytest.fixture` decorator
- Use `@pytest.mark.parametrize` for similar tests with different data
- Clear test function names like `test_create_match_success`
- Arrange-Act-Assert pattern
- Use api_client methods (or create them if missing)
- Proper error assertions with `pytest.raises`

Example test structure:
```python
\"\"\"Test suite for /api/matches endpoint\"\"\"
import pytest
from backend.api_client import MatchesClient

@pytest.fixture
def valid_match_data():
    \"\"\"Valid match data for creation tests\"\"\"
    return {{
        "home_team_id": 1,
        "away_team_id": 2,
        "match_date": "2025-06-15",
        "season_id": 1
    }}

def test_create_match_success(valid_match_data):
    \"\"\"Test successful match creation with valid data\"\"\"
    # Arrange
    client = MatchesClient()

    # Act
    response = client.create_match(valid_match_data)

    # Assert
    assert response.status_code == 201
    assert "id" in response.json()
    assert response.json()["home_team_id"] == valid_match_data["home_team_id"]
```

Generate the FULL test file and write it using the write_file tool.
"""


# Export for agent creation
__all__ = ["create_forge_agent", "get_forge_task_description"]
