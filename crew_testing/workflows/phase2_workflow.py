"""
Phase 2 Workflow - End-to-End Test Generation

Orchestrates the 4-agent workflow:
1. ARCHITECT - Design test scenarios
2. MOCKER - Generate test data
3. FORGE - Generate test code
4. FLASH - Execute tests

Usage:
    from crew_testing.workflows.phase2_workflow import run_phase2_workflow
    result = run_phase2_workflow("/api/matches")
"""

from crewai import Crew, Task, Process
from rich.console import Console

from crew_testing.config import CrewConfig
from crew_testing.agents import (
    create_architect_agent,
    get_architect_task_description,
    create_mocker_agent,
    get_mocker_task_description,
    create_forge_agent,
    get_forge_task_description,
    create_flash_agent,
    get_flash_task_description,
)

console = Console()


def run_phase2_workflow(
    endpoint: str,
    backend_url: str = "http://localhost:8000",
    verbose: bool = False
) -> str:
    """
    Run the complete Phase 2 workflow for an endpoint

    This orchestrates all 4 agents in sequence:
    1. ARCHITECT designs test scenarios
    2. MOCKER generates test data fixtures
    3. FORGE generates pytest test file
    4. FLASH executes tests and reports results

    Args:
        endpoint: API endpoint to test (e.g., "/api/matches")
        backend_url: Base URL of MT backend
        verbose: Show detailed agent conversations

    Returns:
        Final test execution report from FLASH
    """
    # Update config
    CrewConfig.VERBOSE = verbose

    # Create agents
    architect = create_architect_agent()
    mocker = create_mocker_agent()
    forge = create_forge_agent()
    flash = create_flash_agent()

    # Create tasks with sequential dependencies
    # ARCHITECT: Design test scenarios
    task_architect = Task(
        description=f"""
Analyze the {{endpoint}} endpoint and design comprehensive test scenarios.

Your task:
1. Read the OpenAPI spec for {{endpoint}}
2. Understand the endpoint's purpose, parameters, and expected behavior
3. Design test scenarios covering:
   - Happy path (valid requests)
   - Error cases (missing/invalid data)
   - Edge cases (boundaries, empty values)
   - Security concerns (injection, auth)
   - Performance (large data, pagination)

Output a detailed test plan in JSON format with comprehensive scenarios.
Be thorough. Think of scenarios developers wouldn't consider.
""",
        expected_output="JSON test plan with comprehensive scenarios",
        agent=architect,
    )

    # MOCKER: Generate test data (depends on ARCHITECT)
    task_mocker = Task(
        description="""
Based on the test scenarios from ARCHITECT (provided in context), generate realistic test data.

Your task:
1. Review the test scenarios and their test_data_requirements
2. Query the database schema to understand table structures and foreign keys
3. Generate pytest fixtures for each unique data requirement
4. Ensure data is realistic and matches database constraints

Output a JSON mapping of fixture names to fixture definitions.
Be realistic. Use actual team names, dates that make sense, etc.
Query the schema to understand what fields are required and what foreign keys exist.
""",
        expected_output="JSON mapping of fixture names to fixture definitions",
        agent=mocker,
        context=[task_architect],  # Gets ARCHITECT output automatically
    )

    # FORGE: Generate test code (depends on ARCHITECT and MOCKER)
    task_forge = Task(
        description="""
Generate a COMPLETE pytest test file with BOTH fixtures AND test functions.

‚ö†Ô∏è  CRITICAL: Do NOT stop after generating fixtures! You MUST generate test functions!

Step-by-step instructions:

1. **CHECK AUTHENTICATION REQUIREMENTS FIRST**
   Use the openapi_auth_detector tool to check if {{endpoint}} requires authentication:
   - If auth required: Add auth_headers fixture to the file
   - If no auth: Skip auth fixtures

2. Review scenarios from ARCHITECT (in context)

3. Create file header with imports:
   ```python
   \"\"\"Test suite for {{endpoint}} endpoint\"\"\"
   import pytest
   from fastapi.testclient import TestClient
   from backend.app import app

   client = TestClient(app)
   ```

4. If authentication is required, add this fixture:
   ```python
   @pytest.fixture
   def auth_headers():
       \"\"\"Mock authentication headers for testing.\"\"\"
       return {{"Authorization": "Bearer mock_token_for_testing"}}
   ```

5. Generate test functions with proper authentication:

   **For endpoints WITHOUT auth:**
   ```python
   def test_get_version_success():
       \"\"\"Test successful GET request\"\"\"
       response = client.get("/api/version")
       assert response.status_code == 200
       assert "version" in response.json()
   ```

   **For endpoints WITH auth:**
   ```python
   def test_get_matches_success(auth_headers):
       \"\"\"Test successful GET request with authentication\"\"\"
       response = client.get("/api/matches", headers=auth_headers)
       assert response.status_code == 200
       assert isinstance(response.json(), list)

   def test_get_matches_unauthorized():
       \"\"\"Test GET request without authentication fails\"\"\"
       response = client.get("/api/matches")
       assert response.status_code == 403  # Forbidden without auth
   ```

6. Write ONE test function per scenario from ARCHITECT
7. Keep tests simple: request ‚Üí assert status ‚Üí assert response data
8. Use write_file tool to save to `backend/tests/test_<name>.py`

REQUIREMENTS:
- ‚úÖ Use openapi_auth_detector tool to check auth requirements
- ‚úÖ Add auth_headers fixture if endpoint requires authentication
- ‚úÖ Pass headers=auth_headers to authenticated requests
- ‚úÖ Include a test that verifies auth is required (403 without headers)
- ‚úÖ Minimum 3 test functions
- ‚úÖ Each test: docstring, client request, 2+ assertions
- ‚úÖ Use client.get(), client.post(), client.put(), client.delete()
- ‚úÖ NO complex setup - keep it simple!

Output JSON: {{"file_path": "backend/tests/test_matches.py", "total_tests": 8, "requires_auth": true}}
""",
        expected_output="Test file path with confirmation that test functions were generated",
        agent=forge,
        context=[task_architect, task_mocker],  # Gets both outputs automatically
    )

    # FLASH: Execute tests (depends on FORGE)
    task_flash = Task(
        description="""
Execute the generated pytest tests and report results.

Your task:
1. Run pytest on the generated test file (from FORGE in context)
2. Parse the test results
3. Report on passes, failures, errors
4. Track coverage metrics
5. Provide actionable feedback on failures

Output a comprehensive test report.
If all tests pass, celebrate! üéâ
If tests fail, provide ACTIONABLE feedback - not just "test failed" but WHY and HOW TO FIX.
""",
        expected_output="Comprehensive test execution report with coverage metrics",
        agent=flash,
        context=[task_forge],  # Gets FORGE output automatically
    )

    # Create crew with sequential process
    crew = Crew(
        agents=[architect, mocker, forge, flash],
        tasks=[task_architect, task_mocker, task_forge, task_flash],
        process=Process.sequential,  # Execute in order
        verbose=verbose,
    )

    # Execute workflow
    console.print(f"\n[bold blue]üöÄ Starting Phase 2 workflow for {endpoint}[/bold blue]\n")
    console.print("[cyan]üìö ARCHITECT ‚Üí üé® MOCKER ‚Üí üîß FORGE ‚Üí ‚ö° FLASH[/cyan]\n")

    result = crew.kickoff(inputs={
        "endpoint": endpoint,
        "backend_url": backend_url,
    })

    return str(result)


# Export for CLI usage
__all__ = ["run_phase2_workflow"]
