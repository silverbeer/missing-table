name: User Journey Tests

on:
  workflow_dispatch:
    inputs:
      environment:
        description: 'Target environment'
        required: true
        default: 'production'
        type: choice
        options:
          - 'production'

env:
  PYTHON_VERSION: '3.13'
  S3_BUCKET: 'quality-missingtable-com'
  CLOUDFRONT_DISTRIBUTION_ID: 'E15AUGI7OKJ99L'

jobs:
  journey-tests:
    name: Run User Journey Tests
    runs-on: [self-hosted, quality-runner]
    timeout-minutes: 20

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Verify Allure CLI
        run: |
          allure --version

      - name: Setup Python with uv
        run: |
          cd backend
          uv python install 3.13
          uv sync --python 3.13

      - name: Run TSC Journey Tests
        id: journey-tests
        env:
          BASE_URL: https://api.missingtable.com
          TSC_PREFIX: tsc_ci_
          TSC_EXISTING_ADMIN_USERNAME: ${{ secrets.TSC_ADMIN_USERNAME }}
          TSC_EXISTING_ADMIN_PASSWORD: ${{ secrets.TSC_ADMIN_PASSWORD }}
        run: |
          cd backend
          mkdir -p allure-results
          uv run pytest tests/tsc/ \
            -v \
            -n 0 \
            --no-cov \
            --alluredir=allure-results \
            --ignore=tests/tsc/test_99_cleanup.py \
            || echo "TESTS_FAILED=true" >> $GITHUB_ENV
        continue-on-error: true

      - name: Run Cleanup (always)
        if: always()
        env:
          BASE_URL: https://api.missingtable.com
          TSC_PREFIX: tsc_ci_
          TSC_EXISTING_ADMIN_USERNAME: ${{ secrets.TSC_ADMIN_USERNAME }}
          TSC_EXISTING_ADMIN_PASSWORD: ${{ secrets.TSC_ADMIN_PASSWORD }}
        run: |
          cd backend
          uv run pytest tests/tsc/test_99_cleanup.py -v -n 0 --no-cov --alluredir=allure-results

      - name: Generate Allure Report
        if: always()
        run: |
          cd backend
          allure generate allure-results -o allure-report --clean

      - name: Add dashboard navigation to report
        if: always()
        run: |
          BANNER='<div style="background:#2563eb;color:white;padding:8px 16px;font-family:-apple-system,BlinkMacSystemFont,sans-serif;font-size:14px;position:fixed;top:0;left:0;right:0;z-index:9999;display:flex;align-items:center;gap:8px;"><a href="https://quality.missingtable.com" style="color:white;text-decoration:none;display:flex;align-items:center;gap:8px;"><span style="font-size:18px;">‚Üê</span> Back to Quality Dashboard</a></div><div style="height:40px;"></div>'
          sed -i "s|<body>|<body>${BANNER}|" backend/allure-report/index.html

      - name: Upload to S3
        if: always()
        run: |
          # Upload latest journey report
          aws s3 sync backend/allure-report/ s3://${S3_BUCKET}/latest/missing-table/prod/journey/ \
            --delete \
            --cache-control "max-age=300"

          # Archive run report
          RUN_DATE=$(date -u +"%Y-%m-%d")
          aws s3 sync backend/allure-report/ s3://${S3_BUCKET}/runs/missing-table/prod/${RUN_DATE}/${{ github.run_id }}/journey/ \
            --cache-control "max-age=31536000"

      - name: Download unit test results from S3
        if: always()
        run: |
          # Download existing unit test summaries to include in dashboard
          mkdir -p /tmp/backend-allure/widgets /tmp/data
          aws s3 cp s3://${S3_BUCKET}/latest/missing-table/prod/allure/widgets/summary.json /tmp/backend-allure/widgets/summary.json || true
          aws s3 cp s3://${S3_BUCKET}/latest/missing-table/prod/data/backend-coverage.json /tmp/data/backend-coverage.json || true
          aws s3 cp s3://${S3_BUCKET}/latest/missing-table/prod/data/frontend-results.json /tmp/data/frontend-results.json || true
          aws s3 cp s3://${S3_BUCKET}/latest/missing-table/prod/data/frontend-coverage.json /tmp/data/frontend-coverage.json || true

      - name: Update dashboard with journey stats
        if: always()
        run: |
          cd backend
          uv run ../scripts/generate-quality-dashboard.py \
            --output /tmp/index.html \
            --commit-sha ${{ github.sha }} \
            --run-id ${{ github.run_id }} \
            --backend-allure-dir /tmp/backend-allure \
            --backend-coverage-json /tmp/data/backend-coverage.json \
            --frontend-results-json /tmp/data/frontend-results.json \
            --frontend-coverage-json /tmp/data/frontend-coverage.json \
            --journey-allure-dir allure-report

          aws s3 cp /tmp/index.html s3://${S3_BUCKET}/index.html \
            --cache-control "max-age=300" \
            --content-type "text/html"

      - name: Invalidate CloudFront
        if: always()
        run: |
          aws cloudfront create-invalidation \
            --distribution-id ${CLOUDFRONT_DISTRIBUTION_ID} \
            --paths "/latest/missing-table/prod/journey/*" "/index.html"

      - name: Summary
        if: always()
        run: |
          # Extract test stats from Allure summary
          PASSED=$(python3 -c "import json; print(json.load(open('backend/allure-report/widgets/summary.json'))['statistic']['passed'])" 2>/dev/null || echo "N/A")
          TOTAL=$(python3 -c "import json; print(json.load(open('backend/allure-report/widgets/summary.json'))['statistic']['total'])" 2>/dev/null || echo "N/A")

          echo "### User Journey Tests Complete!" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Environment:** production (api.missingtable.com)" >> $GITHUB_STEP_SUMMARY
          echo "**Test Prefix:** tsc_ci_" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Results:** ${PASSED}/${TOTAL} tests passed" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Reports:**" >> $GITHUB_STEP_SUMMARY
          echo "- [User Journey Report](https://quality.missingtable.com/latest/missing-table/prod/journey/)" >> $GITHUB_STEP_SUMMARY
          echo "- [Quality Dashboard](https://quality.missingtable.com)" >> $GITHUB_STEP_SUMMARY

      - name: Fail if tests failed
        if: env.TESTS_FAILED == 'true'
        run: exit 1
