"""
Advanced Vulnerability Trending and Remediation Tracking System
Comprehensive vulnerability lifecycle management with ML-powered risk assessment
"""

import asyncio
import json
import logging
import hashlib
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Union, Tuple
from dataclasses import dataclass, asdict
from enum import Enum
import aiohttp
import asyncpg
import numpy as np
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler
import logfire
from prometheus_client import Counter, Histogram, Gauge
import redis
import yaml
from fastapi import FastAPI, HTTPException, BackgroundTasks
from pydantic import BaseModel, Field

logger = logging.getLogger(__name__)

# Prometheus metrics
VULNERABILITIES_TOTAL = Gauge(
    'vulnerabilities_total',
    'Total number of vulnerabilities',
    ['severity', 'status', 'component']
)

VULNERABILITY_AGE_DAYS = Histogram(
    'vulnerability_age_days',
    'Age of vulnerabilities in days',
    ['severity', 'component']
)

REMEDIATION_TIME_HOURS = Histogram(
    'vulnerability_remediation_hours',
    'Time taken to remediate vulnerabilities',
    ['severity', 'component']
)

VULNERABILITY_RISK_SCORE = Gauge(
    'vulnerability_risk_score',
    'ML-calculated vulnerability risk score',
    ['vuln_id']
)

SLA_VIOLATIONS = Counter(
    'vulnerability_sla_violations_total',
    'Number of SLA violations for vulnerability remediation',
    ['severity', 'component']
)

class VulnerabilitySeverity(str, Enum):
    """Vulnerability severity levels"""
    CRITICAL = "critical"
    HIGH = "high"
    MEDIUM = "medium"
    LOW = "low"
    INFO = "info"

class VulnerabilityStatus(str, Enum):
    """Vulnerability lifecycle status"""
    DISCOVERED = "discovered"
    TRIAGED = "triaged"
    IN_PROGRESS = "in_progress"
    AWAITING_DEPLOYMENT = "awaiting_deployment"
    RESOLVED = "resolved"
    ACCEPTED_RISK = "accepted_risk"
    FALSE_POSITIVE = "false_positive"
    WONT_FIX = "wont_fix"

class ComponentType(str, Enum):
    """Component types for vulnerability categorization"""
    CONTAINER_IMAGE = "container_image"
    KUBERNETES_MANIFEST = "kubernetes_manifest"
    APPLICATION_CODE = "application_code"
    DEPENDENCY = "dependency"
    INFRASTRUCTURE = "infrastructure"
    CONFIGURATION = "configuration"

@dataclass
class VulnerabilitySource:
    """Vulnerability source information"""
    scanner: str
    scan_id: str
    scan_timestamp: datetime
    confidence: float
    source_metadata: Dict[str, Any]

@dataclass
class CVSS:
    """Common Vulnerability Scoring System data"""
    version: str  # "2.0" or "3.1"
    base_score: float
    temporal_score: Optional[float]
    environmental_score: Optional[float]
    vector: str
    exploitability_score: float
    impact_score: float

@dataclass
class VulnerabilityDetails:
    """Detailed vulnerability information"""
    vuln_id: str
    cve_id: Optional[str]
    title: str
    description: str
    severity: VulnerabilitySeverity
    component: str
    component_type: ComponentType
    affected_versions: List[str]
    fixed_versions: List[str]
    cvss: Optional[CVSS]
    cwe_id: Optional[str]
    references: List[str]
    discovered_date: datetime
    source: VulnerabilitySource
    
@dataclass
class RemediationPlan:
    """Vulnerability remediation plan"""
    vuln_id: str
    strategy: str  # "patch", "update", "configuration", "mitigation"
    estimated_effort_hours: float
    priority_score: float
    business_impact: str
    technical_complexity: str
    dependencies: List[str]
    timeline: Dict[str, datetime]  # milestones
    assigned_team: str
    verification_steps: List[str]

@dataclass
class VulnerabilityTrend:
    """Vulnerability trend data point"""
    timestamp: datetime
    total_count: int
    by_severity: Dict[str, int]
    by_component: Dict[str, int]
    by_status: Dict[str, int]
    average_age_days: float
    sla_compliance_rate: float
    risk_score_distribution: Dict[str, float]

class VulnerabilityTracker:
    """Advanced vulnerability tracking and analytics system"""
    
    def __init__(self):
        self.db_pool: Optional[asyncpg.Pool] = None
        self.redis_client = redis.Redis(host='localhost', port=6379, db=2)
        self.ml_model: Optional[RandomForestRegressor] = None
        self.scaler = StandardScaler()
        self.sla_thresholds = {
            VulnerabilitySeverity.CRITICAL: 24,  # 24 hours
            VulnerabilitySeverity.HIGH: 72,      # 3 days
            VulnerabilitySeverity.MEDIUM: 168,   # 7 days
            VulnerabilitySeverity.LOW: 720       # 30 days
        }
        
    async def initialize(self):
        """Initialize vulnerability tracking system"""
        self.db_pool = await asyncpg.create_pool(
            "postgresql://user:password@localhost/vulnerabilities",
            min_size=5,
            max_size=20
        )
        await self._create_tables()
        await self._initialize_ml_models()
        
        logfire.info("Vulnerability tracking system initialized")
    
    async def _create_tables(self):
        """Create vulnerability tracking database tables"""
        async with self.db_pool.acquire() as conn:
            await conn.execute("""
                CREATE TABLE IF NOT EXISTS vulnerabilities (
                    vuln_id VARCHAR PRIMARY KEY,
                    cve_id VARCHAR,
                    title TEXT NOT NULL,
                    description TEXT,
                    severity VARCHAR NOT NULL,
                    status VARCHAR NOT NULL DEFAULT 'discovered',
                    component VARCHAR NOT NULL,
                    component_type VARCHAR NOT NULL,
                    affected_versions JSONB,
                    fixed_versions JSONB,
                    cvss_data JSONB,
                    cwe_id VARCHAR,
                    references JSONB,
                    discovered_date TIMESTAMP NOT NULL,
                    resolved_date TIMESTAMP,
                    source_data JSONB,
                    risk_score FLOAT,
                    business_impact VARCHAR,
                    technical_complexity VARCHAR,
                    created_at TIMESTAMP DEFAULT NOW(),
                    updated_at TIMESTAMP DEFAULT NOW()
                );
                
                CREATE TABLE IF NOT EXISTS remediation_plans (
                    plan_id VARCHAR PRIMARY KEY,
                    vuln_id VARCHAR REFERENCES vulnerabilities(vuln_id),
                    strategy VARCHAR NOT NULL,
                    estimated_effort_hours FLOAT,
                    priority_score FLOAT,
                    business_impact TEXT,
                    technical_complexity VARCHAR,
                    dependencies JSONB,
                    timeline JSONB,
                    assigned_team VARCHAR,
                    verification_steps JSONB,
                    status VARCHAR DEFAULT 'draft',
                    created_at TIMESTAMP DEFAULT NOW(),
                    updated_at TIMESTAMP DEFAULT NOW()
                );
                
                CREATE TABLE IF NOT EXISTS vulnerability_trends (
                    id SERIAL PRIMARY KEY,
                    timestamp TIMESTAMP NOT NULL,
                    total_count INTEGER,
                    by_severity JSONB,
                    by_component JSONB,
                    by_status JSONB,
                    average_age_days FLOAT,
                    sla_compliance_rate FLOAT,
                    risk_score_distribution JSONB,
                    created_at TIMESTAMP DEFAULT NOW()
                );
                
                CREATE TABLE IF NOT EXISTS vulnerability_scans (
                    scan_id VARCHAR PRIMARY KEY,
                    scanner VARCHAR NOT NULL,
                    scan_type VARCHAR NOT NULL,
                    target VARCHAR NOT NULL,
                    started_at TIMESTAMP NOT NULL,
                    completed_at TIMESTAMP,
                    status VARCHAR DEFAULT 'running',
                    vulnerabilities_found INTEGER DEFAULT 0,
                    scan_metadata JSONB,
                    created_at TIMESTAMP DEFAULT NOW()
                );
                
                CREATE TABLE IF NOT EXISTS sla_violations (
                    violation_id VARCHAR PRIMARY KEY,
                    vuln_id VARCHAR REFERENCES vulnerabilities(vuln_id),
                    severity VARCHAR NOT NULL,
                    threshold_hours INTEGER NOT NULL,
                    actual_hours FLOAT NOT NULL,
                    violation_date TIMESTAMP NOT NULL,
                    resolved_date TIMESTAMP,
                    impact_assessment TEXT,
                    created_at TIMESTAMP DEFAULT NOW()
                );
                
                -- Indexes for performance
                CREATE INDEX IF NOT EXISTS idx_vulnerabilities_severity ON vulnerabilities(severity);
                CREATE INDEX IF NOT EXISTS idx_vulnerabilities_status ON vulnerabilities(status);
                CREATE INDEX IF NOT EXISTS idx_vulnerabilities_component ON vulnerabilities(component);
                CREATE INDEX IF NOT EXISTS idx_vulnerabilities_discovered_date ON vulnerabilities(discovered_date);
                CREATE INDEX IF NOT EXISTS idx_vulnerability_trends_timestamp ON vulnerability_trends(timestamp);
                CREATE INDEX IF NOT EXISTS idx_sla_violations_severity ON sla_violations(severity);
            """)
    
    async def _initialize_ml_models(self):
        """Initialize machine learning models for risk assessment"""
        # Initialize Random Forest model for risk scoring
        self.ml_model = RandomForestRegressor(
            n_estimators=100,
            random_state=42,
            max_depth=10
        )
        
        # Train initial model with synthetic data if no historical data exists
        await self._train_risk_model()
    
    async def _train_risk_model(self):
        """Train ML model for vulnerability risk assessment"""
        try:
            # Get historical vulnerability data
            async with self.db_pool.acquire() as conn:
                data = await conn.fetch("""
                    SELECT 
                        severity,
                        component_type,
                        cvss_data,
                        business_impact,
                        technical_complexity,
                        EXTRACT(EPOCH FROM (resolved_date - discovered_date))/3600 as resolution_time_hours
                    FROM vulnerabilities 
                    WHERE resolved_date IS NOT NULL
                    AND cvss_data IS NOT NULL
                """)
            
            if len(data) < 10:  # Need minimum training data
                # Use synthetic training data
                data = self._generate_synthetic_training_data()
            
            # Prepare features and labels
            features = []
            labels = []
            
            for row in data:
                feature_vector = self._extract_features(row)
                if feature_vector and 'resolution_time_hours' in row:
                    features.append(feature_vector)
                    labels.append(row['resolution_time_hours'])
            
            if features:
                # Scale features
                features_scaled = self.scaler.fit_transform(features)
                
                # Train model
                self.ml_model.fit(features_scaled, labels)
                
                logfire.info(
                    "ML risk model trained",
                    training_samples=len(features),
                    model_score=self.ml_model.score(features_scaled, labels)
                )
        
        except Exception as e:
            logger.error(f"Failed to train risk model: {e}")
    
    def _generate_synthetic_training_data(self) -> List[Dict[str, Any]]:
        """Generate synthetic training data for model initialization"""
        synthetic_data = []
        
        # Generate diverse vulnerability scenarios
        severities = ["critical", "high", "medium", "low"]
        components = ["container_image", "application_code", "dependency"]
        
        for i in range(100):
            severity = np.random.choice(severities)
            component = np.random.choice(components)
            
            # Simulate realistic resolution times based on severity
            if severity == "critical":
                resolution_time = np.random.exponential(24)  # Avg 24 hours
            elif severity == "high":
                resolution_time = np.random.exponential(72)  # Avg 72 hours
            elif severity == "medium":
                resolution_time = np.random.exponential(168)  # Avg 7 days
            else:
                resolution_time = np.random.exponential(720)  # Avg 30 days
            
            cvss_score = np.random.uniform(1.0, 10.0)
            
            synthetic_data.append({
                'severity': severity,
                'component_type': component,
                'cvss_data': {'base_score': cvss_score},
                'business_impact': 'medium',
                'technical_complexity': 'medium',
                'resolution_time_hours': resolution_time
            })
        
        return synthetic_data
    
    def _extract_features(self, vuln_data: Dict[str, Any]) -> Optional[List[float]]:
        """Extract features for ML model"""
        try:
            # Severity encoding
            severity_encoding = {
                'critical': 4,
                'high': 3,
                'medium': 2,
                'low': 1,
                'info': 0
            }
            
            # Component type encoding
            component_encoding = {
                'container_image': 3,
                'application_code': 2,
                'dependency': 1,
                'configuration': 0
            }
            
            # Extract CVSS score
            cvss_score = 0.0
            if vuln_data.get('cvss_data'):
                cvss_score = vuln_data['cvss_data'].get('base_score', 0.0)
            
            # Business impact encoding
            impact_encoding = {
                'critical': 4,
                'high': 3,
                'medium': 2,
                'low': 1,
                'none': 0
            }
            
            # Technical complexity encoding
            complexity_encoding = {
                'very_high': 4,
                'high': 3,
                'medium': 2,
                'low': 1,
                'very_low': 0
            }
            
            features = [
                severity_encoding.get(vuln_data.get('severity', 'low'), 1),
                component_encoding.get(vuln_data.get('component_type', 'configuration'), 0),
                cvss_score,
                impact_encoding.get(vuln_data.get('business_impact', 'medium'), 2),
                complexity_encoding.get(vuln_data.get('technical_complexity', 'medium'), 2)
            ]
            
            return features
        
        except Exception as e:
            logger.error(f"Error extracting features: {e}")
            return None
    
    @logfire.instrument("Record Vulnerability")
    async def record_vulnerability(self, vuln: VulnerabilityDetails) -> str:
        """Record a new vulnerability"""
        with logfire.span("vulnerability_recording", vuln_id=vuln.vuln_id):
            # Calculate risk score using ML model
            risk_score = await self._calculate_risk_score(vuln)
            
            # Store vulnerability
            async with self.db_pool.acquire() as conn:
                await conn.execute("""
                    INSERT INTO vulnerabilities 
                    (vuln_id, cve_id, title, description, severity, component, 
                     component_type, affected_versions, fixed_versions, cvss_data,
                     cwe_id, references, discovered_date, source_data, risk_score)
                    VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13, $14, $15)
                    ON CONFLICT (vuln_id) DO UPDATE SET
                        updated_at = NOW(),
                        risk_score = EXCLUDED.risk_score
                """, 
                    vuln.vuln_id, vuln.cve_id, vuln.title, vuln.description,
                    vuln.severity.value, vuln.component, vuln.component_type.value,
                    json.dumps(vuln.affected_versions), json.dumps(vuln.fixed_versions),
                    json.dumps(asdict(vuln.cvss)) if vuln.cvss else None,
                    vuln.cwe_id, json.dumps(vuln.references), vuln.discovered_date,
                    json.dumps(asdict(vuln.source)), risk_score
                )
            
            # Update metrics
            VULNERABILITIES_TOTAL.labels(
                severity=vuln.severity.value,
                status=VulnerabilityStatus.DISCOVERED.value,
                component=vuln.component_type.value
            ).inc()
            
            VULNERABILITY_RISK_SCORE.labels(vuln_id=vuln.vuln_id).set(risk_score)
            
            # Check SLA
            await self._check_sla_compliance(vuln)
            
            logfire.info(
                "Vulnerability recorded",
                vuln_id=vuln.vuln_id,
                severity=vuln.severity.value,
                component=vuln.component,
                risk_score=risk_score
            )
            
            return vuln.vuln_id
    
    async def _calculate_risk_score(self, vuln: VulnerabilityDetails) -> float:
        """Calculate ML-based risk score for vulnerability"""
        try:
            if not self.ml_model:
                # Fallback to simple scoring
                return self._simple_risk_score(vuln)
            
            # Extract features
            features = self._extract_features({
                'severity': vuln.severity.value,
                'component_type': vuln.component_type.value,
                'cvss_data': asdict(vuln.cvss) if vuln.cvss else None,
                'business_impact': 'medium',  # Default
                'technical_complexity': 'medium'  # Default
            })
            
            if not features:
                return self._simple_risk_score(vuln)
            
            # Scale features and predict
            features_scaled = self.scaler.transform([features])
            predicted_resolution_time = self.ml_model.predict(features_scaled)[0]
            
            # Convert resolution time to risk score (0-100)
            # Higher resolution time = higher risk
            max_hours = 720  # 30 days
            risk_score = min(100, (predicted_resolution_time / max_hours) * 100)
            
            return round(risk_score, 2)
        
        except Exception as e:
            logger.error(f"Error calculating ML risk score: {e}")
            return self._simple_risk_score(vuln)
    
    def _simple_risk_score(self, vuln: VulnerabilityDetails) -> float:
        """Simple risk scoring fallback"""
        base_scores = {
            VulnerabilitySeverity.CRITICAL: 90,
            VulnerabilitySeverity.HIGH: 70,
            VulnerabilitySeverity.MEDIUM: 50,
            VulnerabilitySeverity.LOW: 30
        }
        
        score = base_scores.get(vuln.severity, 30)
        
        # Adjust based on CVSS if available
        if vuln.cvss:
            cvss_adjustment = (vuln.cvss.base_score / 10) * 10
            score = (score + cvss_adjustment) / 2
        
        return round(min(100, score), 2)
    
    async def _check_sla_compliance(self, vuln: VulnerabilityDetails):
        """Check SLA compliance for vulnerability"""
        threshold_hours = self.sla_thresholds.get(vuln.severity, 720)
        current_age_hours = (datetime.now() - vuln.discovered_date).total_seconds() / 3600
        
        if current_age_hours > threshold_hours:
            # SLA violation
            violation_id = f"sla_{vuln.vuln_id}_{int(datetime.now().timestamp())}"
            
            async with self.db_pool.acquire() as conn:
                await conn.execute("""
                    INSERT INTO sla_violations 
                    (violation_id, vuln_id, severity, threshold_hours, 
                     actual_hours, violation_date)
                    VALUES ($1, $2, $3, $4, $5, $6)
                """, 
                    violation_id, vuln.vuln_id, vuln.severity.value,
                    threshold_hours, current_age_hours, datetime.now()
                )
            
            # Update metrics
            SLA_VIOLATIONS.labels(
                severity=vuln.severity.value,
                component=vuln.component_type.value
            ).inc()
            
            logfire.warn(
                "SLA violation detected",
                vuln_id=vuln.vuln_id,
                severity=vuln.severity.value,
                threshold_hours=threshold_hours,
                actual_hours=current_age_hours
            )
    
    @logfire.instrument("Update Vulnerability Status")
    async def update_vulnerability_status(self, vuln_id: str, 
                                        new_status: VulnerabilityStatus,
                                        resolution_notes: Optional[str] = None):
        """Update vulnerability status"""
        with logfire.span("status_update", vuln_id=vuln_id, status=new_status.value):
            async with self.db_pool.acquire() as conn:
                # Get current vulnerability
                vuln = await conn.fetchrow("""
                    SELECT * FROM vulnerabilities WHERE vuln_id = $1
                """, vuln_id)
                
                if not vuln:
                    raise ValueError(f"Vulnerability {vuln_id} not found")
                
                # Update status
                update_fields = {
                    'status': new_status.value,
                    'updated_at': datetime.now()
                }
                
                if new_status == VulnerabilityStatus.RESOLVED:
                    update_fields['resolved_date'] = datetime.now()
                    
                    # Calculate remediation time
                    discovered_date = vuln['discovered_date']
                    remediation_hours = (datetime.now() - discovered_date).total_seconds() / 3600
                    
                    # Update metrics
                    REMEDIATION_TIME_HOURS.labels(
                        severity=vuln['severity'],
                        component=vuln['component_type']
                    ).observe(remediation_hours)
                
                # Build dynamic UPDATE query
                set_clause = ', '.join([f"{field} = ${i+2}" for i, field in enumerate(update_fields.keys())])
                query = f"UPDATE vulnerabilities SET {set_clause} WHERE vuln_id = $1"
                values = [vuln_id] + list(update_fields.values())
                
                await conn.execute(query, *values)
                
                # Update Prometheus metrics
                VULNERABILITIES_TOTAL.labels(
                    severity=vuln['severity'],
                    status=vuln['status'],
                    component=vuln['component_type']
                ).dec()
                
                VULNERABILITIES_TOTAL.labels(
                    severity=vuln['severity'],
                    status=new_status.value,
                    component=vuln['component_type']
                ).inc()
            
            logfire.info(
                "Vulnerability status updated",
                vuln_id=vuln_id,
                old_status=vuln['status'],
                new_status=new_status.value
            )
    
    @logfire.instrument("Generate Vulnerability Trends")
    async def generate_vulnerability_trends(self, days_back: int = 30) -> List[VulnerabilityTrend]:
        """Generate vulnerability trend data"""
        with logfire.span("trend_generation", days_back=days_back):
            trends = []
            end_date = datetime.now()
            
            for i in range(days_back):
                date = end_date - timedelta(days=i)
                trend = await self._calculate_trend_for_date(date)
                trends.append(trend)
            
            # Store trends
            await self._store_trends(trends)
            
            return list(reversed(trends))  # Return chronological order
    
    async def _calculate_trend_for_date(self, date: datetime) -> VulnerabilityTrend:
        """Calculate vulnerability trend for a specific date"""
        async with self.db_pool.acquire() as conn:
            # Total count
            total_count = await conn.fetchval("""
                SELECT COUNT(*) FROM vulnerabilities 
                WHERE discovered_date <= $1 
                AND (resolved_date IS NULL OR resolved_date > $1)
            """, date)
            
            # By severity
            severity_data = await conn.fetch("""
                SELECT severity, COUNT(*) as count
                FROM vulnerabilities 
                WHERE discovered_date <= $1 
                AND (resolved_date IS NULL OR resolved_date > $1)
                GROUP BY severity
            """, date)
            
            by_severity = {row['severity']: row['count'] for row in severity_data}
            
            # By component
            component_data = await conn.fetch("""
                SELECT component_type, COUNT(*) as count
                FROM vulnerabilities 
                WHERE discovered_date <= $1 
                AND (resolved_date IS NULL OR resolved_date > $1)
                GROUP BY component_type
            """, date)
            
            by_component = {row['component_type']: row['count'] for row in component_data}
            
            # By status
            status_data = await conn.fetch("""
                SELECT status, COUNT(*) as count
                FROM vulnerabilities 
                WHERE discovered_date <= $1 
                AND (resolved_date IS NULL OR resolved_date > $1)
                GROUP BY status
            """, date)
            
            by_status = {row['status']: row['count'] for row in status_data}
            
            # Average age
            avg_age = await conn.fetchval("""
                SELECT AVG(EXTRACT(EPOCH FROM ($1 - discovered_date))/86400)
                FROM vulnerabilities 
                WHERE discovered_date <= $1 
                AND (resolved_date IS NULL OR resolved_date > $1)
            """, date) or 0.0
            
            # SLA compliance rate
            total_vulns = await conn.fetchval("""
                SELECT COUNT(*) FROM vulnerabilities 
                WHERE discovered_date <= $1
            """, date)
            
            sla_violations = await conn.fetchval("""
                SELECT COUNT(*) FROM sla_violations 
                WHERE violation_date <= $1
            """, date)
            
            sla_compliance_rate = 100.0
            if total_vulns > 0:
                sla_compliance_rate = ((total_vulns - (sla_violations or 0)) / total_vulns) * 100
            
            # Risk score distribution
            risk_data = await conn.fetch("""
                SELECT 
                    CASE 
                        WHEN risk_score >= 80 THEN 'high_risk'
                        WHEN risk_score >= 60 THEN 'medium_risk'
                        WHEN risk_score >= 40 THEN 'low_risk'
                        ELSE 'minimal_risk'
                    END as risk_category,
                    COUNT(*) as count
                FROM vulnerabilities 
                WHERE discovered_date <= $1 
                AND (resolved_date IS NULL OR resolved_date > $1)
                AND risk_score IS NOT NULL
                GROUP BY risk_category
            """, date)
            
            risk_distribution = {row['risk_category']: row['count'] for row in risk_data}
            
            return VulnerabilityTrend(
                timestamp=date,
                total_count=total_count or 0,
                by_severity=by_severity,
                by_component=by_component,
                by_status=by_status,
                average_age_days=round(avg_age, 2),
                sla_compliance_rate=round(sla_compliance_rate, 2),
                risk_score_distribution=risk_distribution
            )
    
    async def _store_trends(self, trends: List[VulnerabilityTrend]):
        """Store vulnerability trends in database"""
        async with self.db_pool.acquire() as conn:
            for trend in trends:
                await conn.execute("""
                    INSERT INTO vulnerability_trends 
                    (timestamp, total_count, by_severity, by_component, by_status,
                     average_age_days, sla_compliance_rate, risk_score_distribution)
                    VALUES ($1, $2, $3, $4, $5, $6, $7, $8)
                    ON CONFLICT (timestamp) DO UPDATE SET
                        total_count = EXCLUDED.total_count,
                        by_severity = EXCLUDED.by_severity,
                        by_component = EXCLUDED.by_component,
                        by_status = EXCLUDED.by_status,
                        average_age_days = EXCLUDED.average_age_days,
                        sla_compliance_rate = EXCLUDED.sla_compliance_rate,
                        risk_score_distribution = EXCLUDED.risk_score_distribution
                """, 
                    trend.timestamp, trend.total_count,
                    json.dumps(trend.by_severity), json.dumps(trend.by_component),
                    json.dumps(trend.by_status), trend.average_age_days,
                    trend.sla_compliance_rate, json.dumps(trend.risk_score_distribution)
                )

# Global vulnerability tracker instance
vulnerability_tracker = VulnerabilityTracker()

# FastAPI integration
class VulnerabilityAPI:
    """FastAPI endpoints for vulnerability tracking"""
    
    def __init__(self, app: FastAPI):
        self.app = app
        self._setup_routes()
    
    def _setup_routes(self):
        @self.app.get("/api/vulnerabilities/trends")
        async def get_vulnerability_trends(days: int = 30):
            """Get vulnerability trends"""
            trends = await vulnerability_tracker.generate_vulnerability_trends(days)
            return {"trends": [asdict(trend) for trend in trends]}
        
        @self.app.get("/api/vulnerabilities/metrics")
        async def get_vulnerability_metrics():
            """Get current vulnerability metrics"""
            async with vulnerability_tracker.db_pool.acquire() as conn:
                # Current counts
                total = await conn.fetchval("SELECT COUNT(*) FROM vulnerabilities WHERE status != 'resolved'")
                by_severity = await conn.fetch("""
                    SELECT severity, COUNT(*) as count 
                    FROM vulnerabilities 
                    WHERE status != 'resolved'
                    GROUP BY severity
                """)
                
                # SLA compliance
                sla_violations = await conn.fetchval("SELECT COUNT(*) FROM sla_violations WHERE resolved_date IS NULL")
                
                return {
                    "total_vulnerabilities": total,
                    "by_severity": {row['severity']: row['count'] for row in by_severity},
                    "active_sla_violations": sla_violations
                }
        
        @self.app.post("/api/vulnerabilities/{vuln_id}/status")
        async def update_status(vuln_id: str, status_data: dict):
            """Update vulnerability status"""
            new_status = VulnerabilityStatus(status_data['status'])
            notes = status_data.get('notes')
            
            await vulnerability_tracker.update_vulnerability_status(vuln_id, new_status, notes)
            return {"status": "updated"}

# Initialize vulnerability tracking
async def initialize_vulnerability_tracking():
    """Initialize vulnerability tracking system"""
    await vulnerability_tracker.initialize()
    logfire.info("Vulnerability tracking system ready")