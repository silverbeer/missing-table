# Security Incident Response Automation System
# This system provides automated incident response capabilities

apiVersion: v1
kind: Namespace
metadata:
  name: incident-response
  labels:
    name: incident-response
    security.missing-table.io/level: "critical"
    compliance.missing-table.io/framework: "soc2"
---
# Incident Response Configuration
apiVersion: v1
kind: ConfigMap
metadata:
  name: incident-response-config
  namespace: incident-response
data:
  config.yaml: |
    # Incident response automation configuration
    incident_response:
      # Severity levels and response times
      severity_levels:
        critical:
          response_time_minutes: 5
          escalation_minutes: 15
          auto_actions: ["isolate", "collect_evidence", "notify_oncall"]
          requires_approval: false
        
        high:
          response_time_minutes: 15
          escalation_minutes: 60
          auto_actions: ["quarantine", "collect_logs", "notify_team"]
          requires_approval: false
        
        medium:
          response_time_minutes: 60
          escalation_minutes: 240
          auto_actions: ["log_event", "create_ticket"]
          requires_approval: true
        
        low:
          response_time_minutes: 240
          escalation_minutes: 1440
          auto_actions: ["log_event"]
          requires_approval: true
      
      # Incident types and response playbooks
      incident_types:
        privilege_escalation:
          severity: "critical"
          playbook: "privilege-escalation-response.md"
          auto_actions:
            - "terminate_pod"
            - "isolate_node"
            - "collect_audit_logs"
            - "create_forensic_image"
        
        data_exfiltration:
          severity: "critical"
          playbook: "data-exfiltration-response.md"
          auto_actions:
            - "block_network_egress"
            - "isolate_workload"
            - "collect_network_logs"
            - "notify_data_protection_team"
        
        malware_detection:
          severity: "high"
          playbook: "malware-response.md"
          auto_actions:
            - "quarantine_container"
            - "scan_for_iocs"
            - "collect_container_image"
            - "update_threat_intel"
        
        compliance_violation:
          severity: "medium"
          playbook: "compliance-violation-response.md"
          auto_actions:
            - "create_compliance_ticket"
            - "notify_compliance_team"
            - "schedule_remediation"
      
      # Communication channels
      notifications:
        slack:
          critical_channel: "#incident-response"
          high_channel: "#security-alerts"
          medium_channel: "#security-notices"
          webhook_secret: "slack-incident-webhook"
        
        pagerduty:
          critical_service: "security-incidents"
          high_service: "security-alerts"
          api_key_secret: "pagerduty-api-key"
        
        email:
          critical_list: ["security-team@missing-table.io", "ciso@missing-table.io"]
          high_list: ["security-team@missing-table.io"]
          medium_list: ["security-team@missing-table.io"]
      
      # Evidence collection
      evidence_collection:
        enabled: true
        storage_bucket: "missing-table-incident-evidence"
        retention_days: 2555  # 7 years for compliance
        encryption: true
        
        collect_types:
          - "pod_logs"
          - "container_images"
          - "network_pcaps"
          - "audit_logs"
          - "system_metrics"
          - "file_system_snapshots"

  playbooks.yaml: |
    # Incident response playbooks
    playbooks:
      privilege-escalation-response.md: |
        # Privilege Escalation Incident Response
        
        ## Immediate Actions (0-5 minutes)
        1. **ISOLATE**: Terminate suspicious pod immediately
        2. **CONTAIN**: Block network access from affected node
        3. **ALERT**: Notify security team and on-call engineer
        4. **PRESERVE**: Take node snapshot for forensics
        
        ## Investigation Phase (5-30 minutes)
        1. **Collect Evidence**:
           - Pod logs and events
           - Container runtime logs
           - Node system logs
           - Network traffic captures
        
        2. **Analyze Attack Vector**:
           - Review container security context
           - Check for privilege escalation exploits
           - Examine syscall patterns
           - Verify image provenance
        
        3. **Impact Assessment**:
           - Determine scope of compromise
           - Check for lateral movement
           - Identify affected resources
           - Assess data exposure risk
        
        ## Containment Actions
        ```bash
        # Isolate affected node
        kubectl taint node <node-name> incident=privilege-escalation:NoSchedule
        kubectl taint node <node-name> incident=privilege-escalation:NoExecute
        
        # Collect evidence
        kubectl logs <pod-name> -n <namespace> > /evidence/pod-logs.txt
        crictl logs <container-id> > /evidence/runtime-logs.txt
        
        # Network isolation
        kubectl apply -f - <<EOF
        apiVersion: networking.k8s.io/v1
        kind: NetworkPolicy
        metadata:
          name: incident-isolation
          namespace: <namespace>
        spec:
          podSelector:
            matchLabels:
              incident: "privilege-escalation"
          policyTypes:
          - Ingress
          - Egress
        EOF
        ```
        
        ## Recovery Actions
        1. **Remediation**:
           - Remove compromised workloads
           - Update security policies
           - Patch vulnerabilities
           - Rotate credentials
        
        2. **Validation**:
           - Verify threat elimination
           - Test security controls
           - Confirm policy enforcement
        
        ## Post-Incident
        1. **Documentation**: Complete incident report
        2. **Lessons Learned**: Update playbooks and policies
        3. **Training**: Brief team on new threats

      data-exfiltration-response.md: |
        # Data Exfiltration Incident Response
        
        ## Immediate Actions (0-5 minutes)
        1. **BLOCK**: Implement network egress blocking
        2. **ISOLATE**: Quarantine affected workloads
        3. **ALERT**: Notify security and data protection teams
        4. **PRESERVE**: Capture network traffic for analysis
        
        ## Investigation Phase
        1. **Network Analysis**:
           - Review VPC flow logs
           - Analyze DNS queries
           - Check for unusual outbound connections
           - Identify data transfer patterns
        
        2. **Data Assessment**:
           - Determine what data was accessed
           - Classify sensitivity level
           - Calculate potential impact
           - Check for encryption status
        
        ## Containment
        ```bash
        # Block egress traffic
        kubectl apply -f - <<EOF
        apiVersion: networking.k8s.io/v1
        kind: NetworkPolicy
        metadata:
          name: block-egress
          namespace: <namespace>
        spec:
          podSelector: {}
          policyTypes:
          - Egress
          egress: []
        EOF
        
        # Isolate workload
        kubectl patch deployment <deployment> -p '{"spec":{"template":{"metadata":{"labels":{"quarantine":"true"}}}}}'
        ```

      malware-response.md: |
        # Malware Detection Incident Response
        
        ## Immediate Actions
        1. **QUARANTINE**: Move container to quarantine namespace
        2. **SCAN**: Run comprehensive malware scan
        3. **ISOLATE**: Block network communications
        4. **PRESERVE**: Save container image for analysis
        
        ## Analysis
        1. **Static Analysis**: Scan container image
        2. **Dynamic Analysis**: Monitor runtime behavior
        3. **IOC Extraction**: Identify indicators of compromise
        4. **Threat Intel**: Update threat intelligence feeds

---
# Incident Response Service Account
apiVersion: v1
kind: ServiceAccount
metadata:
  name: incident-responder
  namespace: incident-response
  annotations:
    security.missing-table.io/description: "Service account for automated incident response"
automountServiceAccountToken: true

---
# Incident Response ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: incident-responder
  labels:
    security.missing-table.io/component: "incident-response"
rules:
# Full cluster access for incident response
- apiGroups: ["*"]
  resources: ["*"]
  verbs: ["*"]
# Node access for forensics
- apiGroups: [""]
  resources: ["nodes"]
  verbs: ["get", "list", "watch", "patch", "update"]

---
# Incident Response ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: incident-responder
  labels:
    security.missing-table.io/component: "incident-response"
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: incident-responder
subjects:
- kind: ServiceAccount
  name: incident-responder
  namespace: incident-response

---
# Incident Response Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: incident-responder
  namespace: incident-response
  labels:
    app: incident-responder
    security.missing-table.io/component: "incident-response"
spec:
  replicas: 2
  selector:
    matchLabels:
      app: incident-responder
  template:
    metadata:
      labels:
        app: incident-responder
        security.missing-table.io/component: "incident-response"
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8080"
    spec:
      serviceAccountName: incident-responder
      securityContext:
        runAsNonRoot: true
        runAsUser: 65534
        runAsGroup: 65534
        fsGroup: 65534
      containers:
      - name: responder
        image: gcr.io/missing-table-prod/incident-responder:v1.0.0
        imagePullPolicy: Always
        ports:
        - containerPort: 8080
          name: http
          protocol: TCP
        - containerPort: 8090
          name: health
          protocol: TCP
        - containerPort: 9090
          name: webhook
          protocol: TCP
        env:
        - name: CONFIG_PATH
          value: "/config"
        - name: LOG_LEVEL
          value: "info"
        - name: CLUSTER_NAME
          value: "missing-table-prod"
        - name: ENVIRONMENT
          value: "production"
        - name: INCIDENT_STORAGE_BUCKET
          value: "missing-table-incident-evidence"
        - name: SLACK_WEBHOOK_URL
          valueFrom:
            secretKeyRef:
              name: incident-secrets
              key: slack-webhook-url
        - name: PAGERDUTY_API_KEY
          valueFrom:
            secretKeyRef:
              name: incident-secrets
              key: pagerduty-api-key
        volumeMounts:
        - name: config
          mountPath: /config
          readOnly: true
        - name: evidence
          mountPath: /evidence
        - name: tmp
          mountPath: /tmp
        securityContext:
          allowPrivilegeEscalation: false
          runAsNonRoot: true
          runAsUser: 65534
          runAsGroup: 65534
          readOnlyRootFilesystem: true
          capabilities:
            drop:
            - ALL
        resources:
          limits:
            cpu: 500m
            memory: 512Mi
          requests:
            cpu: 200m
            memory: 256Mi
        livenessProbe:
          httpGet:
            path: /health
            port: 8090
          initialDelaySeconds: 30
          periodSeconds: 30
        readinessProbe:
          httpGet:
            path: /ready
            port: 8090
          initialDelaySeconds: 5
          periodSeconds: 10
      volumes:
      - name: config
        projected:
          sources:
          - configMap:
              name: incident-response-config
              items:
              - key: config.yaml
                path: config.yaml
              - key: playbooks.yaml
                path: playbooks.yaml
      - name: evidence
        persistentVolumeClaim:
          claimName: incident-evidence
      - name: tmp
        emptyDir:
          sizeLimit: 1Gi

---
# Evidence Storage PVC
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: incident-evidence
  namespace: incident-response
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 50Gi

---
# Incident Response Service
apiVersion: v1
kind: Service
metadata:
  name: incident-responder
  namespace: incident-response
  labels:
    app: incident-responder
    security.missing-table.io/component: "incident-response"
  annotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "8080"
    prometheus.io/path: "/metrics"
spec:
  type: ClusterIP
  ports:
  - port: 8080
    targetPort: 8080
    protocol: TCP
    name: http
  - port: 9090
    targetPort: 9090
    protocol: TCP
    name: webhook
  selector:
    app: incident-responder

---
# Incident Response Webhook for External Alerts
apiVersion: v1
kind: Service
metadata:
  name: incident-webhook
  namespace: incident-response
  labels:
    app: incident-responder
    security.missing-table.io/component: "webhook"
spec:
  type: LoadBalancer
  ports:
  - port: 443
    targetPort: 9090
    protocol: TCP
    name: webhook-https
  selector:
    app: incident-responder

---
# Forensic Analysis Job Template
apiVersion: batch/v1
kind: Job
metadata:
  name: forensic-analysis-template
  namespace: incident-response
  labels:
    security.missing-table.io/component: "forensics"
spec:
  template:
    metadata:
      labels:
        app: forensic-analysis
    spec:
      serviceAccountName: incident-responder
      restartPolicy: Never
      securityContext:
        runAsNonRoot: true
        runAsUser: 65534
        runAsGroup: 65534
        fsGroup: 65534
      containers:
      - name: forensics
        image: gcr.io/missing-table-prod/forensic-analyzer:v1.0.0
        command:
        - /bin/sh
        - -c
        - |
          echo "🔍 Starting forensic analysis..."
          
          # Set up analysis environment
          export INCIDENT_ID="${INCIDENT_ID:-$(date +%Y%m%d-%H%M%S)}"
          export EVIDENCE_DIR="/evidence/${INCIDENT_ID}"
          mkdir -p "$EVIDENCE_DIR"
          
          echo "Collecting pod logs..."
          if [ -n "$POD_NAME" ] && [ -n "$POD_NAMESPACE" ]; then
            kubectl logs "$POD_NAME" -n "$POD_NAMESPACE" --previous > "$EVIDENCE_DIR/pod-logs.txt" || true
            kubectl logs "$POD_NAME" -n "$POD_NAMESPACE" > "$EVIDENCE_DIR/pod-logs-current.txt" || true
          fi
          
          echo "Collecting pod events..."
          kubectl get events -n "$POD_NAMESPACE" --sort-by=.metadata.creationTimestamp > "$EVIDENCE_DIR/events.txt" || true
          
          echo "Collecting pod manifest..."
          kubectl get pod "$POD_NAME" -n "$POD_NAMESPACE" -o yaml > "$EVIDENCE_DIR/pod-manifest.yaml" || true
          
          echo "Collecting node information..."
          if [ -n "$NODE_NAME" ]; then
            kubectl describe node "$NODE_NAME" > "$EVIDENCE_DIR/node-info.txt" || true
          fi
          
          echo "Collecting network policies..."
          kubectl get networkpolicies -A -o yaml > "$EVIDENCE_DIR/network-policies.yaml" || true
          
          echo "Generating forensic report..."
          cat > "$EVIDENCE_DIR/forensic-report.md" <<EOF
          # Forensic Analysis Report
          
          **Incident ID**: $INCIDENT_ID
          **Timestamp**: $(date)
          **Analyst**: Automated Forensic System
          
          ## Incident Summary
          - **Pod**: $POD_NAME
          - **Namespace**: $POD_NAMESPACE
          - **Node**: $NODE_NAME
          - **Incident Type**: $INCIDENT_TYPE
          
          ## Evidence Collected
          - Pod logs (current and previous)
          - Kubernetes events
          - Pod manifest
          - Node information
          - Network policies
          
          ## Next Steps
          1. Review collected evidence
          2. Correlate with security alerts
          3. Determine root cause
          4. Implement remediation
          
          EOF
          
          echo "Uploading evidence to storage..."
          if command -v gsutil >/dev/null 2>&1; then
            gsutil -m cp -r "$EVIDENCE_DIR" "gs://missing-table-incident-evidence/"
          fi
          
          echo "Forensic analysis completed for incident $INCIDENT_ID"
        env:
        - name: INCIDENT_ID
          value: ""
        - name: POD_NAME
          value: ""
        - name: POD_NAMESPACE
          value: ""
        - name: NODE_NAME
          value: ""
        - name: INCIDENT_TYPE
          value: ""
        volumeMounts:
        - name: evidence
          mountPath: /evidence
        securityContext:
          allowPrivilegeEscalation: false
          runAsNonRoot: true
          runAsUser: 65534
          runAsGroup: 65534
          readOnlyRootFilesystem: true
          capabilities:
            drop:
            - ALL
        resources:
          limits:
            cpu: 500m
            memory: 512Mi
          requests:
            cpu: 200m
            memory: 256Mi
      volumes:
      - name: evidence
        persistentVolumeClaim:
          claimName: incident-evidence

---
# Incident Secrets (to be populated with real values)
apiVersion: v1
kind: Secret
metadata:
  name: incident-secrets
  namespace: incident-response
  labels:
    security.missing-table.io/component: "secrets"
type: Opaque
data:
  # Base64 encoded webhook URLs and API keys
  slack-webhook-url: aHR0cHM6Ly9ob29rcy5zbGFjay5jb20vc2VydmljZXMvVDAwMDAwMDAwL0IwMDAwMDAwMC9YWFhYWFhYWFhYWFhYWFhYWFhYWA==
  pagerduty-api-key: WFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhY
  teams-webhook-url: aHR0cHM6Ly9vdXRsb29rLm9mZmljZS5jb20vd2ViaG9vay9YWFhYWFhYWFhYWFhYWFhYWFhYWA==

---
# Incident Response Metrics
apiVersion: v1
kind: ServiceMonitor
metadata:
  name: incident-responder
  namespace: incident-response
  labels:
    app: incident-responder
spec:
  selector:
    matchLabels:
      app: incident-responder
  endpoints:
  - port: http
    path: /metrics
    interval: 30s

---
# Incident Response Alerts
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: incident-response-alerts
  namespace: incident-response
  labels:
    prometheus: kube-prometheus
    role: alert-rules
spec:
  groups:
  - name: incident.response
    rules:
    - alert: IncidentResponderDown
      expr: up{job="incident-responder"} == 0
      for: 2m
      labels:
        severity: critical
        service: incident-response
      annotations:
        summary: "Incident response system is down"
        description: "Incident response automation has been down for more than 2 minutes"
    
    - alert: IncidentResponseLatency
      expr: histogram_quantile(0.95, rate(incident_response_duration_seconds_bucket[5m])) > 300
      for: 5m
      labels:
        severity: warning
        service: incident-response
      annotations:
        summary: "High incident response latency"
        description: "95th percentile incident response time is {{ $value }} seconds"
    
    - alert: UnresolvedCriticalIncidents
      expr: incident_status{severity="critical",status="open"} > 0
      for: 15m
      labels:
        severity: critical
        service: incident-response
      annotations:
        summary: "Critical incidents unresolved"
        description: "{{ $value }} critical incidents have been open for more than 15 minutes"